AdventureWorks ETL & Data Warehouse ğŸš´â€â™‚ï¸ğŸ“Š

ğŸ“‹ Sobre o Projeto

Este projeto implementa uma soluÃ§Ã£o completa de Engenharia de Dados e Business Intelligence para a base de dados pÃºblica AdventureWorks. O objetivo foi construir um Data Warehouse (DW) robusto utilizando o modelo dimensional (Star Schema), orquestrado automaticamente para facilitar anÃ¡lises de vendas, produtos e performance regional.

A arquitetura foi desenvolvida em contÃªineres para garantir reprodutibilidade e isolamento, simulando um ambiente de produÃ§Ã£o real onde dados transacionais brutos sÃ£o transformados em insights estratÃ©gicos.

âš™ï¸ Arquitetura e Tecnologias

O pipeline de dados segue a arquitetura ELT/ETL moderna:

IngestÃ£o (Staging): Script Python coleta dados brutos (CSV) e carrega no PostgreSQL.

OrquestraÃ§Ã£o: Apache Airflow gerencia as dependÃªncias das tarefas (DAGs).

Processamento: Pandas e SQLAlchemy realizam a limpeza, tipagem e transformaÃ§Ã£o.

Armazenamento (DW): PostgreSQL hospeda o Data Warehouse com tabelas Fato e DimensÃµes.

Modelagem: CriaÃ§Ã£o de chaves artificiais (Surrogate Keys) e integridade referencial (Cascade).

ğŸ—‚ï¸ Modelagem de Dados (Star Schema)

O DW foi modelado para otimizar consultas analÃ­ticas, composto por:

Fato: fato_vendas (MÃ©tricas de vendas, descontos e quantidades).

DimensÃµes:

dim_produto: Detalhes do produto, custos e preÃ§os.

dim_cliente: Dados cadastrais unificados.

dim_territorio: Hierarquia geogrÃ¡fica (PaÃ­s, RegiÃ£o).

dim_tempo: CalendÃ¡rio expandido para anÃ¡lises temporais.

dim_status: NormalizaÃ§Ã£o dos status de pedidos.

ğŸš€ Como Executar

PrÃ©-requisitos

Docker e Docker Compose instalados.

Git.

Passo a Passo

Clone o repositÃ³rio:

git clone [https://github.com/DiegoRezendeB/Data-Warehouse-e-ETL-com-Apache-Airflow.git)


Inicie o ambiente (Airflow + Postgres):

docker compose up -d --build


Prepare a Ã¡rea de Staging (Carga Inicial):
Execute o script localmente para baixar os dados brutos e popular o banco:

# Certifique-se de ter as libs instaladas (pandas, sqlalchemy, psycopg2)
python setup_dados.py


Acesse o Airflow:

URL: http://localhost:8080

UsuÃ¡rio/Senha: admin / admin

Execute a DAG:
Ative a DAG etl_adventureworks_completo e aguarde a conclusÃ£o.

ğŸ“ˆ Indicadores (KPIs)

O projeto permite a extraÃ§Ã£o de mÃ©tricas estratÃ©gicas via SQL ou ferramentas de BI (Power BI), tais como:

ğŸ’° Receita Bruta e LÃ­quida

ğŸ·ï¸ Ticket MÃ©dio por Pedido

ğŸŒ Share de Vendas por PaÃ­s

ğŸ“¦ Curva ABC de Produtos

ğŸ“‰ Margem de Lucro

ğŸ› ï¸ Desafios Superados

Durante o desenvolvimento, foram solucionados problemas complexos de engenharia de dados:

Integridade Referencial: ImplementaÃ§Ã£o de DROP CASCADE e recriaÃ§Ã£o de tabelas para garantir consistÃªncia entre Fato e DimensÃµes.

Tipagem de Dados: Tratamento de erros de casting (Text vs BigInt) provenientes de dados sujos no Staging.

Surrogate Keys: GeraÃ§Ã£o automÃ¡tica de chaves primÃ¡rias (SERIAL) para isolar o DW das chaves de negÃ³cio.

Locale/Idioma: Tratamento de datas em PortuguÃªs via cÃ³digo Python, independente do SO do contÃªiner.
